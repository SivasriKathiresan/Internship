{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0pFrsaIddsl",
        "outputId": "2c0418f2-4c5d-48cf-f4ca-66f9e80f2711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching article from: https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "\n",
            "Generating summary...\n",
            "\n",
            "=== Article Summary ===\n",
            "As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:\n",
            " Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. [57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n",
            " Rule-based systems are commonly used:\n",
            " In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import heapq\n",
        "\n",
        "# Download NLTK resources (only needed first time)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line is added to download the missing resource\n",
        "\n",
        "def fetch_article_text(url):\n",
        "    \"\"\"Fetch article text from a URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "        return article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article: {e}\")\n",
        "        return None\n",
        "\n",
        "def summarize_text(text, num_sentences=5):\n",
        "    \"\"\"Summarize text using extractive summarization\"\"\"\n",
        "    # Tokenize sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) < num_sentences:\n",
        "        return text  # Return original if text is shorter than requested summary\n",
        "\n",
        "    # Remove stopwords and tokenize words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_frequencies = FreqDist(words)\n",
        "\n",
        "    # Calculate sentence scores\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if sentence not in sentence_scores:\n",
        "                    sentence_scores[sentence] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "    # Get top N sentences\n",
        "    summary_sentences = heapq.nlargest(\n",
        "        num_sentences,\n",
        "        sentence_scores,\n",
        "        key=sentence_scores.get\n",
        "    )\n",
        "\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# Example usage with a predefined URL\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change this URL to any article you want to summarize\n",
        "    article_url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "\n",
        "    print(f\"Fetching article from: {article_url}\")\n",
        "    article_text = fetch_article_text(article_url)\n",
        "\n",
        "    if article_text:\n",
        "        print(\"\\nGenerating summary...\")\n",
        "        summary = summarize_text(article_text, num_sentences=5)\n",
        "\n",
        "        print(\"\\n=== Article Summary ===\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"Failed to fetch article text.\")\n"
      ]
    }
  ]
}
